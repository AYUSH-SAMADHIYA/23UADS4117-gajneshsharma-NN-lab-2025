import numpy as np

def perceptron(inputs, weights, bias):
    return np.where(np.dot(inputs, weights) + bias > 0, 1, 0)

def xor_mlp(x):
    # Define weights and biases for the hidden layer perceptrons
    p1_w = np.array([1, -1])  # AND-like perceptron
    p2_w = np.array([-1, 1])  # AND-like perceptron
    p3_w = np.array([1, 1])   # OR perceptron
    p4_w = np.array([-1, -1]) # NOR perceptron
    
    p1_b = 0
    p2_b = 0
    p3_b = -0.5
    p4_b = 0.5
    
    # Compute hidden layer outputs
    h1 = perceptron(x, p1_w, p1_b)
    h2 = perceptron(x, p2_w, p2_b)
    h3 = perceptron(x, p3_w, p3_b)
    h4 = perceptron(x, p4_w, p4_b)
    
    # Combine hidden layer outputs into an array
    hidden_output = np.stack((h1, h2, h3, h4), axis=1)
    
    # XOR perceptron (using h1 and h2, ignoring h3 and h4)
    xor_w = np.array([1, 1, 0, 0])
    xor_b = 0
    
    # Compute final XOR output
    return perceptron(hidden_output, xor_w, xor_b)

# XOR truth table inputs
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
Y = xor_mlp(X)

# Print results
for i in range(len(X)):
    print(f'Input: {X[i]} -> XOR Output: {Y[i]}')
